# https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/docker-image/v0.12/alpine-elasticsearch/conf/kubernetes.conf
# does this work as a comment
@include kubernetes.conf

# Rename log field to message as this is know by Humio
<filter kubernetes.**>
  @type record_modifier
  <record>
    message "${record['log']}"
    k8s "${record['kubernetes']}"
    container_id "${record['docker']['container_id']}"
    disable "${record['kubernetes']['labels']['disable-logging']}"
    my_tag "k8s.${record['kubernetes']['namespace_name']}.${record['kubernetes']['container_name']}"
  </record>
  remove_keys log,kubernetes
</filter>

<filter **>
  @type grep
  <exclude>
    key disable
    pattern ^true$
  </exclude>
</filter>

<filter **>
  @type record_transformer
 remove_keys disable
</filter>

<filter **>
  @type concat
  key message
  stream_identity_key container_id

  multiline_start_regexp /^\[?\d\d\d\d-\d\d-\d\d[T ]\d\d\:\d\d\:\d\d[,\.]\d\d\d\]?/
  timeout_label @TIMEOUT
</filter>

<filter **>
 @type record_transformer
 remove_keys my_tag container_id
</filter>

<label @TIMEOUT>
  <match **>
    @type elasticsearch

    include_tag_key false

    host "#{ENV['FLUENT_HUMIO_HOST']}"
    path "/api/v1/dataspaces/#{ENV['FLUENT_HUMIO_DATA_SPACE']}/ingest/elasticsearch/"
    scheme "https"
    port "443"

    user "#{ENV['FLUENT_HUMIO_INGEST_TOKEN']}"
    password ""

    logstash_format true

    reload_connections "true"
    logstash_prefix "fluentd:kubernetes2humio"
    <buffer>
      queued_chunks_limit_size 32
      chunk_limit_size 500K
      flush_interval 1s
      retry_max_interval 30
      flush_thread_count 8
    </buffer>
  </match>
</label>

<match **>
  @type elasticsearch

  include_tag_key false

  host "#{ENV['FLUENT_HUMIO_HOST']}"
  path "/api/v1/dataspaces/#{ENV['FLUENT_HUMIO_DATA_SPACE']}/ingest/elasticsearch/"
  scheme "https"
  port "443"

  user "#{ENV['FLUENT_HUMIO_INGEST_TOKEN']}"
  password ""

  logstash_format true

  reload_connections "true"
  logstash_prefix "fluentd:kubernetes2humio"
  <buffer>
    queued_chunks_limit_size 32
    chunk_limit_size 500K
    flush_interval 1s
    retry_max_interval 30
    flush_thread_count 8
  </buffer>
</match>
